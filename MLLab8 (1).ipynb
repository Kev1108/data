{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:right;color:navy\">Machine Learning & Data Analysis &nbsp;  &nbsp;  &nbsp; Janurary 7, 2022</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8:  Algorithms for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機器學習演算法\n",
    "\n",
    "我們來討論一下常見的機器學習演算法，這些演算法背後的原理，需要花很多時間理解，我們只做簡單的介紹，並不做推導。今天主要的目的是學習如何使用這些演算法。\n",
    "\n",
    "其實我們在之前的課程中，已經使用過三個演算法，分別是 [k-Nearest Neighbors.](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)、 [Logistic Regression.](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)、 [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)，利用這些演算法，造出估計器(Estimators)，經由訓練資料集(Training set)上的擬合(Fit)，再對測試資料集做預測(Predict)，為了增加選擇性，今天再談幾個演算法。\n",
    "\n",
    "今天討論的機器學習演算法分為兩類：監督學習演算法、非監督學習演算法，監督學習演算法中因目的不同，又有分類演算法及迴歸演算法兩種類型，條列如下：\n",
    "\n",
    "**監督學習 (Supervised algorithm)**  \n",
    "* 分類演算法(Classification algorithm)  \n",
    "  - 邏輯迴歸(Logistic regression)  \n",
    "  - 線性判別分析(Linear discriminant analysis)  \n",
    "  - K鄰近(K nearest neighbors)  \n",
    "  - 樸素貝氏演算法(Naive Bayes)  \n",
    "  - 支持向量機(Support vector machines)  \n",
    "  - 決策樹(Decision tree)  \n",
    "* 迴歸演算法(Regression algorithm)  \n",
    "  - 線性迴歸(Linear regression)  \n",
    "  - 嶺迴歸(Ridge regression)\n",
    "\n",
    "**非監督學習  (Unsupervised algorithm)**\n",
    "* 分群演算法\n",
    "  - K-means  \n",
    "  - 支持向量機(Support vector machines)  \n",
    "  - 最大期望演算法(Maximize expectation)  \n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 監督學習 (Supervised algorithm)\n",
    "### 迴歸演算法\n",
    "我們先開始討論迴歸演算法\n",
    "\n",
    "#### 廣義線性模型(Generalized Linear Models)  \n",
    "在某些狀況下目標值(target value)是各個特徵(Features)(輸入變數(input variables))的線性組合。用數學語言來描述是這樣的： 假設 $\\hat y$ 是目標值的預測值，則有\n",
    "\n",
    "$$\\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p$$\n",
    "\n",
    "在整個模組中, 我們把向量 $w=(w_1,...,w_p)$ 記作 coef_ (係數)，並把 $w_0$ 記作 intercept_ (截距)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 線性迴歸(Linear regression)   \n",
    "[LinearRegression](https://sklearn.apachecn.org/#/docs/master/2)  \n",
    "普通最小二乘法([Ordinary Least Squares](https://en.wikipedia.org/wiki/Ordinary_least_squares))： 擬合一個帶有係數 $w=(w_0,w_1,...,w_p)$ 的線性模型使得資料集實際觀測資料和預測資料（估計值）之間的殘差平方和最小(minimizes the sum of squared residuals,)。 其數學運算式為:  \n",
    "$$\\min_{w} {|| X w - y||_2}^2$$\n",
    "\n",
    "![LinearRegression](https://scikit-learn.org/stable/_images/sphx_glr_plot_ols_001.png)\n",
    "\n",
    "[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) 會調用 fit 方法來擬合資料 $X， y$，並且將線性模型的係數 $w$ 存儲在其成員變數 coef_ 及 intercep_中。:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "這個例題是有關於波士頓房價的迴歸檢測，請嘗試用不同的特徵來做迴歸  \n",
    "[Example](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html)  \n",
    "\n",
    "我們試試使用下列特徵：\n",
    "\n",
    "ZN：25,000平方英尺以上的土地劃為住宅用地的比例。  \n",
    "RM：每個住宅的平均房間數。  \n",
    "AGE：1940年之前建造的自有住房的比例  \n",
    "CHAS：有沒有河流經過 (如果等於1，說明有，等於0就說明沒有)  \n",
    "CRIM：犯罪率   \n",
    "\n",
    "[Boston house prices dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#boston-dataset)\n",
    "\n",
    "#### Step 1\n",
    "下載資料並觀察資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "4       18.7  396.90   5.33  \n",
       "..       ...     ...    ...  \n",
       "501     21.0  391.99   9.67  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "505     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "\n",
    "boston = load_boston()\n",
    "boston_X = boston['data']\n",
    "boston_y = boston['target']\n",
    "features = boston['feature_names']\n",
    "\n",
    "X = pd.DataFrame(boston_X, columns= features)\n",
    "#y = pd.DataFrame(boston_y,columns=['Price'])    # Try this, shape of y will be (506,1), a 2D dataframe. This will give .coef_ a (1,# of features) array!!!!\n",
    "y = pd.Series(boston_y, name='price')\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "選擇要分析的特徵，並觀察資料特徵間的相關性。\n",
    "\n",
    "[Pandas : Computations / descriptive stats](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats)  \n",
    "[DataFrame.corr](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html#pandas.DataFrame.corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>CHAS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN     RM   AGE  CHAS\n",
       "0    0.00632  18.0  6.575  65.2   0.0\n",
       "1    0.02731   0.0  6.421  78.9   0.0\n",
       "2    0.02729   0.0  7.185  61.1   0.0\n",
       "3    0.03237   0.0  6.998  45.8   0.0\n",
       "4    0.06905   0.0  7.147  54.2   0.0\n",
       "..       ...   ...    ...   ...   ...\n",
       "501  0.06263   0.0  6.593  69.1   0.0\n",
       "502  0.04527   0.0  6.120  76.7   0.0\n",
       "503  0.06076   0.0  6.976  91.0   0.0\n",
       "504  0.10959   0.0  6.794  89.3   0.0\n",
       "505  0.04741   0.0  6.030  80.8   0.0\n",
       "\n",
       "[506 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X.isnull().sum()\n",
    "chosen_feat = ['CRIM','ZN','RM','AGE','CHAS']\n",
    "X1=X.loc[:,chosen_feat]\n",
    "X1\n",
    "\n",
    "frame=X1.join(y)\n",
    "frame.corr()\n",
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "使用 Train test split 資料分為訓練集、驗證集，驗證集為十分之一，random_state=42。  \n",
    "使用 LinearRegression 演算法，用 fit 方法來擬合訓練集資料，  \n",
    "用 score 方法計算訓練集的 R2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X1, y, test_size = 0.33, random_state=42)\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4  \n",
    "對驗證集做預測，計算並列印出你的 Explained variance score, MSE, R2 誤差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance score: 0.37\n",
      "Mean squared error: 3696.80\n",
      "Coefficient of determination: 0.36\n"
     ]
    }
   ],
   "source": [
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# The explained_variance_score\n",
    "print('Explained variance score: %.2f'\n",
    "      % explained_variance_score(y_test,y_pred))\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(y_test,y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5  \n",
    "列印出你的迴歸方程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regression equation is: \n",
      " price = -22.38-0.229*CRIM+0.015*ZN+7.631*RM-0.049*AGE+5.488*CHAS\n"
     ]
    }
   ],
   "source": [
    "strings = str(round(clf.intercept_,3))\n",
    "for i in range(len(chosen_feat)):\n",
    "    if reg.coef_[i]>= 0:\n",
    "        word = '+' + str(round(reg.coef_[i],3)) + '*' + chosen_feat[i]\n",
    "    else:\n",
    "        word = '-' + str(round(-reg.coef_[i],3)) + '*' + chosen_feat[i]\n",
    "    strings += word\n",
    "\n",
    "print('The regression equation is: \\n price =',strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下個練習是有關於糖尿病的迴歸檢測，請嘗試用不同的特徵來做迴歸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "利用 datasets.load_diabetes 所提供的資料，使用 [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) 做糖尿病的迴歸檢測，\n",
    "1. 請嘗試用不同的特徵來做迴歸\n",
    "2. 使用 Train test split 資料分為訓練集、檢視集，檢視集為10分之一\n",
    "3. 列印出你的迴歸方程式\n",
    "4. 列印出你的 Explained variance score, MSE, R2 誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果對 'bmi','bp','s1','s2' 做迴歸，random_state=42，應該得到：  \n",
    "The regression equation is $y = 152.146 + 798.162*bmi + 379.811*bp + 191.656*s1 - 175.381*s2$  \n",
    "Explained variance score: 0.44  \n",
    "Mean squared error: 3413.58  \n",
    "Coefficient of determination: 0.44  \n",
    "\n",
    "[Coefficient of determination](https://zh.wikipedia.org/wiki/%E5%86%B3%E5%AE%9A%E7%B3%BB%E6%95%B0)  \n",
    "[Coefficient of determination 2](https://www.britannica.com/science/coefficient-of-determination)  \n",
    "[醫學統計學](https://wangcc.me/LSHTMlearningnote/)  \n",
    "[OLS](https://wangcc.me/LSHTMlearningnote/OLS.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regression equation is: \n",
      " price = 152.146-0.229*bmi+0.015*bp+7.631*s1-0.049*s2\n",
      "Explained variance score: 0.44\n",
      "Mean squared error: 3413.58\n",
      "Coefficient of determination: 0.44\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "diabetes\n",
    "\n",
    "X_diabetes = diabetes['data']\n",
    "y_diabetes = diabetes['target']\n",
    "features = diabetes['feature_names']\n",
    "\n",
    "X= pd.DataFrame(X_diabetes, columns=features)\n",
    "y= pd.Series(y_diabetes)\n",
    "# Use 'bmi','bp','s1','s2' features\n",
    "chosen_feat = ['bmi','bp','s1','s2']\n",
    "\n",
    "# 建立 X 資料\n",
    "X1 = X.loc[:,chosen_feat]\n",
    "\n",
    "# 使用 Train test split 資料分為訓練集、檢視集，檢視集為10分之一\n",
    "X_train,X_test,y_train,y_test = train_test_split(X1, y, test_size = 0.1, random_state = 42)\n",
    "\n",
    "# 建立 linear regression object\n",
    "clf = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# 列印出你的迴歸方程式\n",
    "strings = str(round(clf.intercept_,3))\n",
    "for i in range(len(chosen_feat)):\n",
    "    if reg.coef_[i]>= 0:\n",
    "        word = '+' + str(round(reg.coef_[i],3)) + '*' + chosen_feat[i]\n",
    "    else:\n",
    "        word = '-' + str(round(-reg.coef_[i],3)) + '*' + chosen_feat[i]\n",
    "    strings += word\n",
    "\n",
    "print('The regression equation is: \\n price =',strings)\n",
    "\n",
    "# 列印出你的 Explained variance score, MSE, R2 誤差\n",
    "# The explained_variance_score\n",
    "print('Explained variance score: %.2f'\n",
    "      % explained_variance_score(y_test,y_pred))\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(y_test,y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 嶺迴歸(Ridge regression)  \n",
    "嶺迴歸專門用於共線性資料分析的「有偏估計」迴歸方法,通過放棄最小二乘法的無偏性(Unbias: 在反覆抽樣的情況下,樣本均值的集合的期望等於總體均值),以損失部分資訊、降低精度為代價獲得迴歸係數更為符合實際、更可靠的迴歸方法,對共線性問題和病態資料的擬合要強於最小二乘法,經常用於多維問題與不適定問題(ill-posed problem)。\n",
    "\n",
    "Ridge regression 通過對係數的大小施加懲罰來解決普通最小二乘法(Ordinary Least Squares)的一些問題。 嶺係數最小化的是帶懲罰項的殘差平方和，數學形式如下\n",
    "$$\\min_{w} {{|| X w - y||_2}^2 + \\alpha {||w||_2}^2}$$\n",
    "其中, $\\alpha≥0$ 是一個控制縮減量(amount of shrinkage)的複雜度參數: $\\alpha≥0$ 的值越大, 縮減量就越大，故而線性模型的係數對共線性(collinearity)就越 Robust.  \n",
    "[Ridge regression](https://en.wikipedia.org/wiki/Ridge_regression)\n",
    "\n",
    "![Ridge coefficients as a function of the regularization](https://scikit-learn.org/stable/_images/sphx_glr_plot_ridge_path_001.png)\n",
    "[Ridge coefficients as a function of the regularization](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html#sphx-glr-auto-examples-linear-model-plot-ridge-path-py)\n",
    "\n",
    "請參考  [Ride Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ridge](https://ask.hellobi.com/blog/lsxxx2011/10581)  \n",
    "[Boston](https://zhuanlan.zhihu.com/p/345972548)  \n",
    "[嶺回歸&Lasso回歸](https://kknews.cc/zh-tw/tech/l49o6l2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "這個例題是有關於波士頓房價的嶺迴歸檢測，請嘗試用不同的特徵來做迴歸  \n",
    "\n",
    "我們試試使用下列特徵：\n",
    "\n",
    "ZN：25,000平方英尺以上的土地劃為住宅用地的比例。  \n",
    "RM：每個住宅的平均房間數。  \n",
    "AGE：1940年之前建造的自有住房的比例  \n",
    "CHAS：有沒有河流經過 (如果等於1，說明有，等於0就說明沒有)  \n",
    "CRIM：犯罪率   \n",
    "\n",
    "[Boston house prices dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#boston-dataset)\n",
    "\n",
    "#### Step 1\n",
    "- 下載資料並選擇要分析的特徵\n",
    "- 使用 Train test split 資料分為訓練集、驗證集，驗證集為十分之一，random_state=42。\n",
    "- 使用 [Ride Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) 演算法，用 fit 方法來擬合訓練集資料，\n",
    "- 用 score 方法計算訓練集的 R2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.568648882825215"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "\n",
    "boston = load_boston()\n",
    "boston_X = boston['data']\n",
    "boston_y = boston['target']\n",
    "features = boston['feature_names']\n",
    "\n",
    "X = pd.DataFrame(boston_X, columns= features)\n",
    "#y = pd.DataFrame(boston_y,columns=['Price'])    # Try this, shape of y will be (506,1), a 2D dataframe. This will give .coef_ a (1,# of features) array!!!!\n",
    "y = pd.Series(boston_y, name='price')\n",
    "chosen_feat = ['CRIM','ZN','RM','AGE','CHAS']\n",
    "X1=X.loc[:,chosen_feat]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X1,y, test_size=0.1, random_state=42)\n",
    "clf = Ridge()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "對驗證集做預測，計算並列印出你的 Explained variance score, MSE, R2 誤差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance score: 0.38\n",
      "Mean squared error: 3766.37\n",
      "Coefficient of determination: 0.38\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# The explained_variance_score\n",
    "print('Explained variance score: %.2f'\n",
    "      % explained_variance_score(y_test, y_pred))\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "列印出你的迴歸方程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regression equation is: \n",
      " price = -22.38-0.229*CRIM+0.015*ZN+7.631*RM-0.049*AGE+5.488*CHAS\n"
     ]
    }
   ],
   "source": [
    "strings = str(round(clf.intercept_,3))\n",
    "for i in range(len(chosen_feat)):\n",
    "    if reg.coef_[i]>= 0:\n",
    "        word = '+' + str(round(reg.coef_[i],3)) + '*' + chosen_feat[i]\n",
    "    else:\n",
    "        word = '-' + str(round(-reg.coef_[i],3)) + '*' + chosen_feat[i]\n",
    "    strings += word\n",
    "\n",
    "print('The regression equation is: \\n price =',strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "利用 datasets.load_diabetes 所提供的資料，使用 [Ridge regression Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) 做糖尿病的回迴歸檢測，\n",
    "1. 請嘗試用不同的特徵來做迴歸\n",
    "2. 使用 Train test split 資料分為訓練集、驗證集，檢視集為十分之一\n",
    "3. 對 $\\alpha$ 值 0.01, 0.1, 0.4, 0.6 做嶺迴歸\n",
    "4. 列印出 $\\alpha$ 的值及其對應迴歸方程式、誤差 Explained variance score,MSE,r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果對 'bmi','bp','s1','s2' 做迴歸，random_state=42，應該得到：  \n",
    "For alpha =  0.01  \n",
    "The regression equation is $y = 152.15 + 788.368*bmi + 380.067*bp + 175.109*s1 - 156.567*s2$  \n",
    "Explained variance score: 0.44  \n",
    "Mean squared error: 3420.85  \n",
    "Coefficient of determination: 0.44  \n",
    "\n",
    "For alpha =  0.1  \n",
    "The regression equation is y = $152.175 + 713.359*bmi + 372.982*bp + 108.749*s1 - 71.133*s2$  \n",
    "Explained variance score: 0.43  \n",
    "Mean squared error: 3466.00  \n",
    "Coefficient of determination: 0.43  \n",
    "\n",
    "For alpha =  0.4  \n",
    "The regression equation is $y = 152.202 + 550.908*bmi + 327.796*bp + 76.09*s1 + 0.486*s2$  \n",
    "Explained variance score: 0.41  \n",
    "Mean squared error: 3635.75  \n",
    "Coefficient of determination: 0.41  \n",
    "\n",
    "For alpha =  0.6  \n",
    "The regression equation is $y = 152.205 + 481.02*bmi + 299.573*bp + 72.551*s1 + 16.544*s2$  \n",
    "Explained variance score: 0.38  \n",
    "Mean squared error: 3766.37  \n",
    "Coefficient of determination: 0.38  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01, 0.1, 0.4, 0.6]\n",
      "for alpha = 0.01\n",
      "The regression equation is: \n",
      " price = 152.15-0.049*s2\n",
      "Explained variance score: 0.44\n",
      "Mean squared error: 3420.85\n",
      "Coefficient of determination: 0.44\n",
      "for alpha = 0.1\n",
      "The regression equation is: \n",
      " price = 152.175-0.049*s2\n",
      "Explained variance score: 0.43\n",
      "Mean squared error: 3466.00\n",
      "Coefficient of determination: 0.43\n",
      "for alpha = 0.4\n",
      "The regression equation is: \n",
      " price = 152.202-0.049*s2\n",
      "Explained variance score: 0.41\n",
      "Mean squared error: 3635.75\n",
      "Coefficient of determination: 0.41\n",
      "for alpha = 0.6\n",
      "The regression equation is: \n",
      " price = 152.205-0.049*s2\n",
      "Explained variance score: 0.38\n",
      "Mean squared error: 3766.37\n",
      "Coefficient of determination: 0.38\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "diabetes\n",
    "\n",
    "X_diabetes = diabetes['data']\n",
    "y_diabetes = diabetes['target']\n",
    "features = diabetes['feature_names']\n",
    "\n",
    "X= pd.DataFrame(X_diabetes, columns=features)\n",
    "y= pd.Series(y_diabetes)\n",
    "# Use 'bmi','bp','s1','s2' features\n",
    "chosen_feat = ['bmi','bp','s1','s2']\n",
    "\n",
    "# 建立 X 資料\n",
    "X1 = X.loc[:,chosen_feat]\n",
    "\n",
    "# 使用 Train test split 資料分為訓練集、檢視集，檢視集為10分之一\n",
    "X_train,X_test,y_train,y_test = train_test_split(X1, y, test_size = 0.10, random_state=42)\n",
    " \n",
    "x = [0.01,0.1,0.4,0.6]\n",
    "print(x)\n",
    "for j in x:\n",
    "    clf = linear_model.Ridge(alpha=j)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    strings = str(round(clf.intercept_,3))\n",
    "    for i in range(len(chosen_feat)):\n",
    "        if reg.coef_[i]>= 0:\n",
    "            word = '+' + str(round(reg.coef_[i],3)) + '*' + chosen_feat[i]\n",
    "        else:\n",
    "            word = '-' + str(round(-reg.coef_[i],3)) + '*' + chosen_feat[i]\n",
    "    strings += word\n",
    "    print('for alpha =', j)\n",
    "    print('The regression equation is: \\n price =',strings)\n",
    "    print('Explained variance score: %.2f'\n",
    "      % explained_variance_score(y_test, y_pred))\n",
    "    print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "    print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test, y_pred))\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類演算法(Classification algorithm)\n",
    "#### 邏輯迴歸(Logistic regression) \n",
    "Logistic迴歸，雖然名字裡有 “迴歸” 二字，但實際上是解決分類問題的一類線性模型。 在某些文獻中，logistic迴歸又被稱作 logit迴歸，最大熵分類(maximum-entropy classification (MaxEnt))，或 對數線性分類器(log-linear classifier)。 在該模型中，使用函數 logistic function 把試驗(trial)的可能的輸出結果建模為概率分佈(Probability distribution):\n",
    "$$p(X)=\\frac{e^{c+Xw}}{1+e^{c+Xw}}$$\n",
    "整理後可以得到\n",
    "$$\\log{\\frac{p(X)}{1-p(X)}}=c+Xw$$\n",
    "方程式左邊被稱為 log-odds 或 logit\n",
    "\n",
    "scikit-learn  [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) 處理正則化(Regulization)問題使用 L1 和 L2 正則化。作為一個優化問題，**<font color='blue'>penalty<font color='black'>** 懲罰項，帶 L2 的二分類 logistic 迴歸要最小化以下代價函數（cost function）：\n",
    "$$\\min_{w, c} \\frac{1}{2}w^T w + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1) $$.\n",
    "類似的, 帶有 L1 正則化的 logistic regression 求解下面的問題：\n",
    "$$\\min_{w, c} \\|w\\|_1 + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1)$$.\n",
    "注意, 在這個記法中, 假定了第 i 次試驗的觀測值 yi 在集合 −1,1 中取值。\n",
    "\n",
    "在 LogisticRegression 使用 **<font color='blue'>solver<font color='black'>** 求解器 : “liblinear”, “newton-cg”, “lbfgs”, “sag” 和 “saga”。實現了這些優化演算法，請參考 [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "[正規化](https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC5-4%E8%AC%9B-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E9%80%B2%E9%9A%8E%E5%AF%A6%E7%94%A8%E6%8A%80%E5%B7%A7-%E6%AD%A3%E8%A6%8F%E5%8C%96-8dd14fcd3140)\n",
    "\n",
    "邏輯迴歸我們已經使用了好幾次，這裡就不再多做練習。邏輯回歸也可以用來做 multi-class 分類，但是我們有下一個方法，在做 multi-class 分類上更方便。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 線性判別分析(Linear discriminant analysis)\n",
    "線性判別分析( [discriminant_analysis.LinearDiscriminantAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) )是一個經典的分類器，找出線性決策平面。他可以很容易計算得到閉式解(即解析解)，天生具有多分類的特性，在實踐中已經被證明很有效，並且沒有超參數要去調節。\n",
    "\n",
    "LDA 分類器的數學化形式  \n",
    "LDA 可以通過簡單的概率模型推導出來，對每一個類別 $k$ 的類條件分佈 $P(X|y=k)$ 進行建模。 有了類條件概率分佈(class conditional distribution)後，預測就可以通過貝葉斯定理所獲得。\n",
    "$$P(y=k | X) = \\frac{P(X | y=k) P(y=k)}{P(X)} = \\frac{P(X | y=k) P(y = k)}{ \\sum_{l} P(X | y=l) \\cdot P(y=l)}$$\n",
    "我們<font color='blue'>選擇能夠使上述條件概率最大化的類別 $k$ 作為預測結果<font color='black'> 。\n",
    "\n",
    "更具體地說，對於線性以及二次判別分析，類條件分佈 $P(X|y)$ 被建模成多變數高斯分佈(multivariate Gaussian distribution), 其密度函數如下所示：\n",
    "$$P(X | y=k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}}\\exp\\left(-\\frac{1}{2} (X-\\mu_k)^t \\Sigma_k^{-1} (X-\\mu_k)\\right)$$\n",
    "其中 $d$ 是特徵分量的個數。\n",
    "為了把該模型作為分類器使用，我們只需要從訓練資料中估計出類的先驗概率 $P(y=k)$ (通過$(y=k)$的樣本的比例得到） 類別均值 $\\mu_k$ （通過經驗樣本的類別均值得到）以及協方差矩陣 (通過經驗樣本的類別協方差或者正則化的估計器 estimator 得到)。\n",
    "\n",
    "在LDA中，假定每個類的高斯分佈與其他所有類共用同一個協方差矩陣 $\\Sigma_k = \\Sigma$ 。這樣一個假設就導致了線性決策面(linear decision surfaces), 我們得到：\n",
    "$$\\log P(y=k | x) = -\\frac{1}{2} (x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k) + \\log P(y = k) + Cst.$$\n",
    "log-posterior of LDA\n",
    "$$\\log P(y=k | x) = \\omega_k^t x + \\omega_{k0} + Cst.$$\n",
    "其中 $\\omega_k = \\Sigma^{-1} \\mu_k$，及 $\\omega_{k0} =\n",
    "-\\frac{1}{2} \\mu_k^t\\Sigma^{-1}\\mu_k + \\log P (y = k)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exapmple\n",
    "在 [Wine recognition dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset) 資料上，使用線性判別分析( LinearDiscriminantAnalysis )分類，請參考:[sklearn.datasets.load_wine](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)。\n",
    "\n",
    "#### Step 1\n",
    "使用 Train test split 資料分為訓練集、驗證集，驗證集為十分之一，random_state=42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "wine = load_wine()\n",
    "\n",
    "X = wine['data']\n",
    "y = wine['target']\n",
    "features = wine['feature_names']\n",
    "\n",
    "# 使用 Train test split 資料分為訓練集、檢視集，檢視集為10分之一\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "對估計量模型 Linear discriminant analysis 做交叉驗證(Cross-Validation)。使用 sklearn.model_selection.cross_validate 對 training set 做交叉驗證，設定 10-fold ， scoring Metrics 使用 accuracy_score、balance_accuracy_score 來評估，將結果列印出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.9774509803921569\n",
      "The balance accuracy is 0.9782539682539684\n"
     ]
    }
   ],
   "source": [
    "# Create linear regression object\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Cross validate\n",
    "scores = cross_validate(clf, X, y, scoring = ['accuracy','balanced_accuracy'], cv=10)\n",
    "print('The accuracy is',scores['test_accuracy'].mean())\n",
    "print('The balance accuracy is',scores['test_balanced_accuracy'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "在驗證集做預測，使用 accuracy_score、balance_accuracy_score 來評估，將結果列印出來。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the test set is 1.0\n",
      "The balance accuracy for the test set is 1.0\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the training sets\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('The accuracy for the test set is',accuracy_score(y_test,y_pred))\n",
    "print('The balance accuracy for the test set is',\n",
    "      balanced_accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "在 Iris Dataset 資料上，\n",
    "1. 使用 Train test split 資料分為訓練集、驗證集，驗證集為十分之一，random_state=42\n",
    "2. 對估計量模型 Linear discriminant analysis 做交叉驗證(Cross-Validation)。使用 [sklearn.model_selection.cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) 對 training set 做交叉驗證，設定 10-fold ， scoring Metrics 使用 accuracy_score、balance_accuracy_score 來評估，將結果列印出來\n",
    "3. 在驗證集做預測，使用 accuracy_score、balance_accuracy_score 來評估，將結果列印出來。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "\n",
    "\n",
    "# 使用 Train test split 資料分為訓練集、檢視集，檢視集為10分之一\n",
    "\n",
    "\n",
    "# Create LinearDiscriminantAnalysis object\n",
    "\n",
    "\n",
    "# Cross validate，將結果列印出來\n",
    "\n",
    "\n",
    "# Train the model using the training sets\n",
    "\n",
    "\n",
    "# Make predictions using the testing set，將結果列印出來。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K鄰近(K nearest neighbors) \n",
    "這方法我們已經使用很多次了，這裡就不再多談。  \n",
    "[KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 樸素貝氏演算法(Naive Bayes) \n",
    "##### Bayes's theorem (貝氏定理)\n",
    "貝氏定理是關於隨機事件A和B的條件概率的一則定理。\n",
    "\n",
    "$${\\displaystyle P(A\\mid B)={\\frac {P(B\\mid A)P(A)}{P(B)}}}$$\n",
    "\n",
    "其中 ${\\displaystyle A}$ 以及 ${\\displaystyle B}$ 為隨機事件，且 ${\\displaystyle P(B)}$ 不為零。${\\displaystyle P(A|B)}$ 是指在事件 ${\\displaystyle B}$ 發生的情況下事件 ${\\displaystyle A}$ 發生的機率。\n",
    "\n",
    "在貝氏定理中，每個名詞都有約定俗成的名稱：\n",
    "\n",
    "> ${\\displaystyle P(A\\,|\\,B)}$ 是已知 ${\\displaystyle B}$ 發生後，${\\displaystyle A}$ 的條件機率。也由於得自 ${\\displaystyle B}$ 的取值而被稱作 ${\\displaystyle A}$ 的後驗機率(Posterior prabability)。 <br>\n",
    "> ${\\displaystyle P(A)})$ 是 ${\\displaystyle A}$ 的先驗機率（Prior probability或邊緣機率）。之所以稱為\"先驗\"是因為它不考慮任何 ${\\displaystyle B}$ 方面的因素。 <br>\n",
    "> ${\\displaystyle P(B\\,|\\,A)}$ 是已知 ${\\displaystyle A}$ 發生後， ${\\displaystyle B}$ 的條件機率。 <br>\n",
    "> ${\\displaystyle P(B)}$ 是 ${\\displaystyle B}$ 的先驗機率。 <br>\n",
    "\n",
    "按這些術語，貝氏定理可表述為： <br>\n",
    "      後驗機率 = (似然性*先驗機率)/標准化常量 <br>\n",
    "也就是說，後驗機率與先驗機率和相似度的乘積成正比。\n",
    "\n",
    "另外，比例 ${\\displaystyle P(B|A)/P(B)}$ 也有時被稱作標准似然度（standardised likelihood），貝氏定理可表述為：\n",
    "\n",
    "後驗機率 = 標准似然度*先驗機率\n",
    "\n",
    "[維基百科-貝氏定理](https://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 應用貝氏定理\n",
    "當我們想用「特徵」(features)來判斷「類別」(labels)時，可以利用「貝氏定理」：\n",
    "\n",
    "$$P(類別~|~{\\rm 特徵}) = \\frac{P({\\rm 特徵}~|~類別)P(類別)}{P({\\rm 特徵})}$$\n",
    "\n",
    "經由計算 Prior probability，$P({\\rm 特徵}~|~類別)、P(類別)、P(特徵)$，可以得到在某個「特徵」下，屬於某「類別」的機率。\n",
    "\n",
    "當利用特徵直接計算類別的機率「$P(類別~|~{\\rm 特徵})$」的演算法，我們稱為 Discrinative learning algorithm，而由計算 Prior probability，$P(特徵~|~類別)、P(類別)、P(特徵)$，可以得到在特徵下，屬於某類別的機率「$P(特徵~|~類別)$」，我們稱為 Generative learning algorithm。在 generative model 中，計算 $P(特徵~|~類別)$ 是很困難的，但經由假設，這個計算過程就簡單多了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes (樸素貝氏演算法)\n",
    "[GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)  \n",
    "假設特徵及類別都是常態分佈，適用於高維度計算。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exapmple\n",
    "在 [Wine recognition dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset) 資料上，使用Gaussian Naive Bayes (樸素貝氏演算法)分類，請參考:[sklearn.datasets.load_wine](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)。\n",
    "\n",
    "#### Step 1\n",
    "使用 Train test split 資料分為訓練集、驗證集，驗證集為十分之一，random_state=42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "wine = \n",
    "\n",
    "X = \n",
    "y = \n",
    "features = \n",
    "\n",
    "# 使用 Train test split 資料分為訓練集、檢視集，檢視集為10分之一\n",
    "X_train,X_test,y_train,y_test = train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "對估計量模型 Linear discriminant analysis 做交叉驗證(Cross-Validation)。使用 sklearn.model_selection.cross_validate 對 training set 做交叉驗證，設定 10-fold ， scoring Metrics 使用 accuracy_score、balance_accuracy_score 來評估，將結果列印出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "clf = \n",
    "\n",
    "# Cross validate\n",
    "scores = cross_validate()\n",
    "print('The accuracy is',scores[].)\n",
    "print('The balance accuracy is',scores[].)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "在驗證集做預測，使用 accuracy_score、balance_accuracy_score 來評估，將結果列印出來。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model using the training sets\n",
    "clf.fit()\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = clf.predict()\n",
    "\n",
    "print('The accuracy for the test set is',accuracy_score())\n",
    "print('The balance accuracy for the test set is',\n",
    "      balanced_accuracy_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "在 Iris Dataset 資料上，\n",
    "1. 使用 Train test split 資料分為訓練集、驗證集，驗證集為十分之一，random_state=42\n",
    "2. 對估計量模型 [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) 做交叉驗證(Cross-Validation)。使用 [sklearn.model_selection.cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) 對 training set 做交叉驗證，設定 10-fold ， scoring Metrics 使用 accuracy_score、balance_accuracy_score 來評估，將結果列印出來\n",
    "3. 在驗證集做預測，使用 accuracy_score、balance_accuracy_score 來評估，將結果列印出來。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "\n",
    "\n",
    "# 使用 Train test split 資料分為訓練集、檢視集，檢視集為10分之一\n",
    "\n",
    "\n",
    "# Create GaussianNB object\n",
    "\n",
    "\n",
    "# Cross validate\n",
    "\n",
    "\n",
    "# Train the model using the training sets，將結果列印出來\n",
    "\n",
    "\n",
    "# Make predictions using the testing set，將結果列印出\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 支持向量機(Support vector machines)\n",
    "\n",
    "支持向量機（SVM）原始的概念，最早是由俄羅斯籍數學家佛拉基米爾‧萬普尼克（Vladimir Naumovich Vapnik）等人於1963年所提出。支持向量機屬於監督學習，可以用來處理分類問題(Classification problems)。\n",
    "\n",
    "如何將一羣屬於不同類別的資料分類？一個非常直覺的做法是用直線、曲線、平面、或曲面把它們分隔開來。\n",
    "\n",
    "但是如何分隔？用直線(平面)、還是曲線(曲面)？\n",
    "\n",
    "怎樣找出最恰當的分隔？那一個直線(平面、曲線、曲面)最好？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 先載入需要的模組。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用[sklearn.datasets.make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs)建立兩組資料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='summer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對以上的資料，可以用直線將它們分開。使用 <br>\n",
    "$\\quad y=x+0.65$ <br>\n",
    "及 <br>\n",
    "$\\quad y=-0.2x+2.9$ <br>\n",
    "都可以。執行下列程式，觀察後，你覺得那一條較好？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "fig, axes = plt.subplots(1,2,figsize=(13,4))\n",
    "\n",
    "for (i,(m,b)) in enumerate([(1, 0.65),(-0.2, 2.9)]):\n",
    "    axes[i].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='summer')\n",
    "    axes[i].text(-1, 2, '$y=%3.1f x+%3.2f$' %(m,b), fontsize=12)\n",
    "    axes[i].plot(xfit, m * xfit + b, '-k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若加入一個新的資料，$(0.6,2.1)$紅色叉叉，你覺得它應該屬於那一類？ <br>\n",
    "執行下列程式，$(0.6,2.1)$用紅色叉叉表示，觀察後，你覺得它應該屬於那一類？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "fig, axes = plt.subplots(1,2,figsize=(13,4))\n",
    "\n",
    "for (i,(m,b)) in enumerate([(1, 0.65),(-0.2, 2.9)]):\n",
    "    axes[i].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='summer')\n",
    "    axes[i].text(-1, 2, '$y=%3.1f x+%3.1f$' %(m,b), fontsize=12)\n",
    "    axes[i].plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "    axes[i].plot(xfit, m * xfit + b, '-k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 對於$\\, y=x+0.65$，我們判定它屬於「綠色」那一類， <br>\n",
    "- 但是對於$\\, y=-0.2x+2.9$，我們判定它屬於「黃色」那一類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有很多選擇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='summer')\n",
    "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "執行下列程式，觀察圖中A、B、C三點。你覺得使用$\\, y=x+0.7$ 當分界線來判斷A、B、C三點的屬性，那一個比較有把握？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "(m,b)=(1, 0.65)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='summer')\n",
    "plt.text(-1, 1, '$y=%3.1f x+%3.1f$' %(m,b), fontsize=12)\n",
    "plt.plot(xfit, m * xfit + b, '-k')\n",
    "plt.text(0, 5.3, 'A', fontsize=15)\n",
    "plt.text(2.8, 0.7, 'B', fontsize=15)\n",
    "plt.text(2.4, 3.4, 'C', fontsize=15)\n",
    "plt.plot([-0.05,0.78],[5.45,1.47],'bo--',[2.75,2.2],[0.75,2.9],'yo--',[2.33,2.42],[3.45,3.1],'ro-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 最有把握，B 次之，C 似乎沒什麼把握。為什麼有這樣的結論？\n",
    "\n",
    "依據 A、B、C 三點到分界線 $\\, y=x+0.7$  的距離來判斷。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根據這個想法，一個理想的分界線，是使得「分界線到這些 training data 的最小距離」最大的那一個！\n",
    "\n",
    "執行下列程式，你覺得那一個分界線最理想？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "fig, axes = plt.subplots(1,3,figsize=(18,4))\n",
    "\n",
    "for (i,(m,b,d)) in enumerate([(1, 0.65, 0.33),(-0.2, 2.9, 0.2),(0.5, 1.6, 0.55)]):\n",
    "    axes[i].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='summer')\n",
    "    axes[i].plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "    yfit = m * xfit + b\n",
    "    axes[i].plot(xfit, yfit, '-k')\n",
    "    axes[i].fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                         color='#AAAAAA', alpha=0.4)\n",
    "    axes[i].text(-1, 2, '$y=%3.1f x+%3.1f$' %(m,b), fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "當然是 $\\, y=0.5x+1.6$ 最好，因為它有最寬的間隔(margin)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用支持向量機（SVM）\n",
    "支持向量機（SVM）就是一個可以讓我們找出最大間隔(margin)的 estimator，其使用語法請參考[sklearn.svm: Support Vector Machines](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)，我們使用其中的[sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)。\n",
    "\n",
    "C : float, optional (default=1.0) <br>\n",
    "Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "\n",
    "kernel : string, optional (default=’rbf’) <br>\n",
    "Specifies the kernel type to be used in the algorithm. It must be one of **‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable**. If none is given, ‘rbf’ will be used.\n",
    "\n",
    "核函数 可以是以下任何形式：:\n",
    "\n",
    "線性: $\\langle x, x'\\rangle$. <br>\n",
    "多项式: $(\\gamma \\langle x, x'\\rangle + r)^d$. d 是關键詞 degree, r 指定 coef0。 <br>\n",
    "rbf: $e^{(-\\gamma \\|x-x'\\|^2)}$. $\\gamma$ 是關键詞 gamma, 必須大於 0。 <br>\n",
    "sigmoid $(\\tanh(\\gamma \\langle x,x'\\rangle + r))$, 其中 r 指定 coef0。 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "給定訓練向量 $x_i \\in \\mathbb{R}^p$, $i=1,…, n$, 和一個向量$ y \\in \\{1, -1\\}^n$, SVC能解決 如下主要問題:\n",
    "\n",
    "$$\\min_{w, b, \\zeta}\\frac{1}{2} w^T w + C\\sum_{i=1}^{n} \\zeta_i \\\\\n",
    "y_i (w^T \\phi (x_i) + b) \\geq 1 - \\zeta_i, \\\\\n",
    " \\zeta_i \\geq 0, i=1, ..., n$$\n",
    "\n",
    "它的對偶是\n",
    "\n",
    "$$\\min_{\\alpha} \\frac{1}{2} \\alpha^T Q \\alpha - e^T \\alpha \\\\\n",
    "y^T \\alpha = 0\\\\\n",
    " 0 \\leq \\alpha_i \\leq C, i=1, ..., n$$\n",
    "\n",
    "其中 $e$ 是所有的向量， $C > 0$ 是上界，$Q$ 是一個 n 由 n 個半正定矩陣， 而 $Q_{ij} \\equiv y_i y_j K(x_i, x_j)$ ，其中 $K(x_i, x_j) = \\phi (x_i)^T \\phi (x_j)$ 是內核。所以訓練向量是通過函數 $\\phi$，間接反映到一個更高維度的（無窮的）空間。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 套用模型\n",
    "使用 linear 線性的核函数(Kernel function)，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "model = SVC(kernel='linear', C=1E10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 訓練 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM 決策函數取決於訓練集的一些子集, 稱作支持向量. 這些支援向量的部分特性可以在屬性 support_vectors_, support_ 和 n_support_ 找到:\n",
    "\n",
    "- support_vectors_: 支援向量(support vectors) <br>\n",
    "- support_ : 支援向量的 indeice <br>\n",
    "- n_support_ : 每一個類別中的支援向量\n",
    "- coef_ : \n",
    "- intercept_:\n",
    "\n",
    "另外SVM 決策函數(Decision function)可在方法 [decision_function(self, X)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.decision_function)找到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請試著了解其意義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.n_support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decision_function(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
